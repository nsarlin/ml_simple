import numpy as np
import sys
from Model import Model
from Utils import *


class LogisticReg(Model):
    """
    Logistic Regression is a simple classification algorithm,
    based on sigmoid function and gradient descent.
    """
    def __init__(self, rate=None, regul=0, num_iter=400, stop_gap=None,
                 loop=False):
        """
        @param rate: Learning Rate. If rate is big, the algorithm
          will learn fast, but can diverge. If rate is small, the
          learning will be slow. If it is left unset, the method
          find_rate will be called to try to set it appropriatly.
        @param regul: Regularization parameter. This parameter
          controls how we want to penalize complex sets of parameters
          generated by the regression. Complex sets of parameters are more
          prone to overfitting (when the model have good results on the
          training set but does not generalize to new examples).
        @param num_iter: How many step of gradient descent should we
          perform.
        @param stop_gap: Instead of setting num_iter, we can decide to
          stop learning when the cost reduces less than stop_gap
          between two iterations.
        """
        self.fixed_rate = rate
        self.regul = regul
        self.num_iter = num_iter
        self.stop_gap = stop_gap
        super().__init__(loop)

    def hypothesis(self, theta, x):
        """
        Here, the hypothesis answers to the question:
        Given our dataset x, what is the probability that x belongs
        to the right class ?
        """
        return sigmoid(np.dot(theta.transpose(), x))

    def cost_vect(self, theta, X, y):
        """
        Vectorized implementation of cost function.
        """
        m = X.shape[0]
        ucost0 = np.dot(np.log(sigmoid(np.dot(X, theta))).transpose(), y)
        a = np.log(np.ones((m,1)) -\
                   sigmoid(np.dot(X, theta))).transpose()
        b = (np.ones((m,1))-y)
        ucost1 = np.dot(a, b)
        ucost = ucost0 + ucost1
        if self.regul != 0:
            ucost -= self.regul/2 * sum(np.square(theta[1:]))

        return -1.0/m*(ucost)

    def cost_loop(self, theta, X, y):
        """
        With sigmoid, we cannot simply compute cost with difference
        between the hypothesis and the target, because target is
        either 0 or 1.
        The log is used to take account of the exponential in the
        hypothesis.
        """
        acc = 0
        m = X.shape[0]

        for i in range(m):
            if y[i] == 1:
                acc += -np.log(self.hypothesis(theta, X[i]))
            else:
                acc += -np.log(1-self.hypothesis(theta, X[i]))
        return acc/m + self.regul/(2*m) * sum(np.square(theta[1:]))

    def grad_vect(self, theta, X, y):
        """
        Computes one step of gradient descent using vectorized
        algorithm.
        """

        m = X.shape[0]
        y = y.reshape((m, 1))
        if self.regul != 0:
            n = theta.shape[0]
            multvec = np.insert(np.ones((n-1, 1))*self.regul/m, 0, 0).transpose()
            multvec = multvec.reshape((n, 1))
            theta -= (self.rate/m)*np.dot(X.transpose(),
                                          sigmoid(np.dot(X, theta)) - y) +\
                      np.multiply(theta, multvec)
        else:
            theta -= (self.rate/m)*np.dot(X.transpose(),
                                          sigmoid(np.dot(X, theta)) - y)
        return theta


    def grad_loop(self, theta, X, y):
        """
        Computes one step of gradient descent using looping algorithm.
        """
        theta_copy = theta.copy()
        n = theta.size
        m = X.shape[0]
        for j in range(n):
            acc = 0
            for i in range(m):
                acc += (self.hypothesis(theta_copy, X[i])-y[i])*X[i,j]
            theta[j] = theta_copy[j] - self.rate*acc/m
            if self.regul != 0 and j != 0:
                theta[j] -= self.regul/m*theta_copy[j]
        return theta

    def grad(self, theta, X, y):
        if self.loop:
            return self.grad_loop(theta, X, y)
        else:
            return self.grad_vect(theta, X, y)

    def grad_descent(self, X, y, init_theta=None):
        """
        Tries to minimize the cost function. Each iteration will make
        a small change in each component of theta, in order to make
        the global cost decrease. The changes are applied using the
        partial derivatives of the cost with respect to each component
        of theta, and rate, the learning rate.
        """
        if self.num_iter is None and self.stop_gap is None:
            raise ValueError("You should either set num_iter or"
                             " stop_gap")
        if init_theta is None:
            init_theta = np.zeros((X.shape[1], 1))

        theta = init_theta

        if self.num_iter:
            pc = 0
            print("{} %".format(pc), end="")
            for i in range(self.num_iter):
                theta = self.grad(theta, X, y)
                if int(100*i/self.num_iter) > pc:
                    pc = int(100*i/self.num_iter)
                    print("\r{} %".format(pc), end="")
            print()
        else:
            last_cost = self.cost(theta, X, y)
            while True:
                theta = self.grad(theta, X, y)
                cost = self.cost(theta, X, y)
                if abs(last_cost - cost) < self.stop_gap:
                    break
                last_cost = cost

        print("Cost: {}".format(self.cost(theta, X, y)))
        return theta


    def find_rate(self, X, y, init_rate = 0.01):
        """
        Tries to find the best learning rate (rate).
        """
        if self.fixed_rate is not None:
            self.rate = fixed_rate
            return self.rate
        self.rate = init_rate
        good = False
        theta = np.zeros((X.shape[1], 1))

        init_cost = self.cost(theta, X, y)
        while True:
            theta = np.zeros((X.shape[1] , 1))
            theta = self.grad(theta, X, y)
            cost = self.cost(theta, X, y)
            if np.isnan(cost) or cost >= init_cost:
                self.rate /= 3
                if good:
                    break
                good = False
            else:
                if not good:
                    break
                self.rate *= 3
                good = True

        print("Rate: {}".format(self.rate))
        return self.rate


    def train(self, X, y, nb_classes=2):
        m, n = X.shape

        if nb_classes == 2:
            if self.rate is None:
                self.find_rate(X, y)
            Theta = self.grad_descent(X, y)
        elif nb_classes > 2:
            Theta = np.zeros((nb_classes, n))
            for c in range(nb_classes):
                print("Class {}".format(c))
                yc = np.array([1 if val == c else 0 for val in y])
                yc = yc.reshape((m, 1))
                self.find_rate(X, yc)
                Theta[c] = self.grad_descent(X, yc).reshape((n,))
        else:
            raise ValueError("Number of classes should be >= 2")
        return Theta

